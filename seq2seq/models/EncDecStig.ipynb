{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformer.torchsm import BaseLayer\n",
    "from torchsm import StigmergicMemoryLayer\n",
    "\n",
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n",
    "    \n",
    "    \n",
    "class StigmergicMemoryLayer_encoder(BaseLayer):\n",
    "    def __init__(self, inputs, space_dim, **kwargs):\n",
    "        BaseLayer.__init__(self, inputs, space_dim)\n",
    "\n",
    "        self.hidden_layers = kwargs[\"hidden_layers\"] if \"hidden_layers\" in kwargs else 0\n",
    "        self.hidden_dim = kwargs[\"hidden_dim\"] if \"hidden_layers\" in kwargs else None\n",
    "\n",
    "        self.n_inputs = inputs\n",
    "        self.space_dim = self.n_outputs\n",
    "\n",
    "        self.init_space = torch.zeros(self.space_dim)\n",
    "\n",
    "        self.mark_net = torch.nn.Sequential()\n",
    "        self.tick_net = torch.nn.Sequential()\n",
    "\n",
    "        if self.hidden_layers != 0:\n",
    "            self.mark_net.add_module(\"input_w\", torch.nn.Linear(self.n_inputs, self.hidden_dim))\n",
    "            self.tick_net.add_module(\"input_w\", torch.nn.Linear(self.n_inputs, self.hidden_dim))\n",
    "            \n",
    "            self.mark_net.add_module(\"input_s\", torch.nn.PReLU())\n",
    "            self.tick_net.add_module(\"input_s\", torch.nn.PReLU())\n",
    "            \n",
    "            for i in range(0, self.hidden_layers-1):\n",
    "                self.mark_net.add_module(\"l\"+str(i)+\"_w\", torch.nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "                self.tick_net.add_module(\"l\"+str(i)+\"_w\", torch.nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "                \n",
    "                self.mark_net.add_module(\"l\"+str(i)+\"_s\", torch.nn.PReLU())\n",
    "                self.tick_net.add_module(\"l\"+str(i)+\"_s\", torch.nn.PReLU())\n",
    "            \n",
    "            self.mark_net.add_module(\"output_w\", torch.nn.Linear(self.hidden_dim, self.space_dim))\n",
    "            self.tick_net.add_module(\"output_w\", torch.nn.Linear(self.hidden_dim, self.space_dim))\n",
    "\n",
    "            self.mark_net.add_module(\"output_relu\", torch.nn.PReLU())\n",
    "            self.tick_net.add_module(\"output_relu\", torch.nn.PReLU())\n",
    "        else:\n",
    "            self.mark_net.add_module(\"linear\", torch.nn.Linear(self.n_inputs, self.space_dim))\n",
    "            self.tick_net.add_module(\"linear\", torch.nn.Linear(self.n_inputs, self.space_dim))\n",
    "\n",
    "            self.mark_net.add_module(\"output_relu\", torch.nn.PReLU())\n",
    "            self.tick_net.add_module(\"output_relu\", torch.nn.PReLU())\n",
    "            \n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def forward(self, input_mark, input_tick = None):\n",
    "\n",
    "        mark = self.mark_net(input_mark)\n",
    "        tick = self.tick_net(input_tick if input_tick is not None else input_mark)\n",
    "        self.space = self.clamp(self.space + mark - tick)\n",
    "\n",
    "        return self.space, mark, tick\n",
    "\n",
    "    def reset(self):\n",
    "        self.space = self.init_space.clone()\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        self = BaseLayer.to(self, *args, **kwargs)\n",
    "        \n",
    "        self.space = self.space.to(*args, **kwargs)\n",
    "        self.init_space = self.init_space.to(*args, **kwargs)\n",
    "        self.mark_net = self.mark_net.to(*args, **kwargs)\n",
    "        self.tick_net = self.tick_net.to(*args, **kwargs)\n",
    "\n",
    "        return self\n",
    "\n",
    "class SRNN_Encoder(BaseLayer):\n",
    "    def __init__(self, input, output, **kwargs):\n",
    "        BaseLayer.__init__(self, input, output, **kwargs)\n",
    "        self.hidden_layers = kwargs[\"hidden_layers\"] if \"hidden_layers\" in kwargs else 0\n",
    "        self.hidden_dim = kwargs[\"hidden_dim\"] if \"hidden_dim\" in kwargs else 30\n",
    "        self.stig_dim = output\n",
    "\n",
    "        self.stigmem = StigmergicMemoryLayer_encoder(\n",
    "            input + self.stig_dim,\n",
    "            self.stig_dim,\n",
    "            hidden_layers=self.hidden_layers,\n",
    "            hidden_dim=self.hidden_dim\n",
    "        )\n",
    "\n",
    "        self.normalization_layer_mark = torch.nn.Linear(self.stig_dim, self.stig_dim)\n",
    "        self.normalization_layer_tick = torch.nn.Linear(self.stig_dim, self.stig_dim)\n",
    "\n",
    "        #self.init_recurrent = torch.zeros(1, self.stig_dim)\n",
    "        self.init_recurrent = torch.ones(1, self.stig_dim)\n",
    "        self.reset()\n",
    "    \n",
    "    def forward(self, input):\n",
    "\n",
    "        self.out, mark, tick = self.stigmem(\n",
    "            torch.cat(\n",
    "                (input, self.normalization_layer_mark(self.recurrent.expand(input.shape[0], self.stig_dim)))\n",
    "            ,1),\n",
    "            torch.cat(\n",
    "                (input, self.normalization_layer_tick(self.recurrent.expand(input.shape[0], self.stig_dim)))\n",
    "            ,1),\n",
    "        )\n",
    "        \n",
    "        return self.out, mark, tick\n",
    "    \n",
    "    def reset(self):\n",
    "        self.recurrent = self.init_recurrent.clone()\n",
    "        self.stigmem.reset()\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        self = BaseLayer.to(self, *args, **kwargs)\n",
    "        \n",
    "        self.stigmem = self.stigmem.to(*args, **kwargs)\n",
    "        self.normalization_layer_mark = self.normalization_layer_mark.to(*args, **kwargs)\n",
    "        self.normalization_layer_tick = self.normalization_layer_tick.to(*args, **kwargs)\n",
    "\n",
    "        self.init_recurrent = self.init_recurrent.to(*args, **kwargs)\n",
    "        self.recurrent = self.recurrent.to(*args, **kwargs)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "class StigmergicMemoryLayer_decoder(BaseLayer):\n",
    "    def __init__(self, inputs, space_dim, **kwargs):\n",
    "        BaseLayer.__init__(self, inputs, space_dim)\n",
    "\n",
    "        self.hidden_layers = kwargs[\"hidden_layers\"] if \"hidden_layers\" in kwargs else 0\n",
    "        self.hidden_dim = kwargs[\"hidden_dim\"] if \"hidden_layers\" in kwargs else None\n",
    "\n",
    "        self.n_inputs = inputs\n",
    "        self.space_dim = self.n_outputs\n",
    "\n",
    "        self.init_space = torch.zeros(self.space_dim)\n",
    "\n",
    "        self.mark_net = torch.nn.Sequential()\n",
    "        self.tick_net = torch.nn.Sequential()\n",
    "\n",
    "        if self.hidden_layers != 0:\n",
    "            self.mark_net.add_module(\"input_w\", torch.nn.Linear(self.n_inputs, self.hidden_dim))\n",
    "            self.tick_net.add_module(\"input_w\", torch.nn.Linear(self.n_inputs, self.hidden_dim))\n",
    "            \n",
    "            self.mark_net.add_module(\"input_s\", torch.nn.PReLU())\n",
    "            self.tick_net.add_module(\"input_s\", torch.nn.PReLU())\n",
    "            \n",
    "            for i in range(0, self.hidden_layers-1):\n",
    "                self.mark_net.add_module(\"l\"+str(i)+\"_w\", torch.nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "                self.tick_net.add_module(\"l\"+str(i)+\"_w\", torch.nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "                \n",
    "                self.mark_net.add_module(\"l\"+str(i)+\"_s\", torch.nn.PReLU())\n",
    "                self.tick_net.add_module(\"l\"+str(i)+\"_s\", torch.nn.PReLU())\n",
    "            \n",
    "            self.mark_net.add_module(\"output_w\", torch.nn.Linear(self.hidden_dim, self.space_dim))\n",
    "            self.tick_net.add_module(\"output_w\", torch.nn.Linear(self.hidden_dim, self.space_dim))\n",
    "\n",
    "            self.mark_net.add_module(\"output_relu\", torch.nn.PReLU())\n",
    "            self.tick_net.add_module(\"output_relu\", torch.nn.PReLU())\n",
    "        else:\n",
    "            self.mark_net.add_module(\"linear\", torch.nn.Linear(self.n_inputs, self.space_dim))\n",
    "            self.tick_net.add_module(\"linear\", torch.nn.Linear(self.n_inputs, self.space_dim))\n",
    "\n",
    "            self.mark_net.add_module(\"output_relu\", torch.nn.PReLU())\n",
    "            self.tick_net.add_module(\"output_relu\", torch.nn.PReLU())\n",
    "            \n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def forward(self, input_mark, input_tick = None):\n",
    "\n",
    "        mark = self.mark_net(input_mark)\n",
    "        tick = self.tick_net(input_tick if input_tick is not None else input_mark)\n",
    "        self.space = self.clamp(self.space + mark - tick)\n",
    "        \n",
    "        return self.space, mark, tick\n",
    "\n",
    "    def reset(self):\n",
    "        self.space = self.init_space.clone()\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        self = BaseLayer.to(self, *args, **kwargs)\n",
    "        \n",
    "        self.space = self.space.to(*args, **kwargs)\n",
    "        self.init_space = self.init_space.to(*args, **kwargs)\n",
    "        self.mark_net = self.mark_net.to(*args, **kwargs)\n",
    "        self.tick_net = self.tick_net.to(*args, **kwargs)\n",
    "\n",
    "        return self\n",
    "\n",
    "class SRNN_Decoder(BaseLayer):\n",
    "    def __init__(self, input, output, **kwargs):\n",
    "        BaseLayer.__init__(self, input, output, **kwargs)\n",
    "        self.hidden_layers = kwargs[\"hidden_layers\"] if \"hidden_layers\" in kwargs else 0\n",
    "        self.hidden_dim = kwargs[\"hidden_dim\"] if \"hidden_dim\" in kwargs else 30\n",
    "        self.stig_dim = output\n",
    "\n",
    "        self.stigmem = StigmergicMemoryLayer_decoder(\n",
    "            input + self.stig_dim,\n",
    "            self.stig_dim,\n",
    "            hidden_layers=self.hidden_layers,\n",
    "            hidden_dim=self.hidden_dim\n",
    "        )\n",
    "\n",
    "        self.normalization_layer_mark = torch.nn.Linear(self.stig_dim, self.stig_dim)\n",
    "        self.normalization_layer_tick = torch.nn.Linear(self.stig_dim, self.stig_dim)\n",
    "\n",
    "        self.init_recurrent = torch.zeros(1, self.stig_dim)\n",
    "        self.reset()\n",
    "    \n",
    "    def forward(self, deposit, removal):\n",
    "\n",
    "        self.out, mark, tick = self.stigmem(\n",
    "            torch.cat(\n",
    "                (deposit, self.normalization_layer_mark(self.recurrent.expand(deposit.shape[0], self.stig_dim)))\n",
    "            ,1),\n",
    "            torch.cat(\n",
    "                (removal, self.normalization_layer_tick(self.recurrent.expand(removal.shape[0], self.stig_dim)))\n",
    "            ,1),\n",
    "        )\n",
    "        \n",
    "        return self.out, mark, tick\n",
    "    \n",
    "    def reset(self):\n",
    "        self.recurrent = self.init_recurrent.clone()\n",
    "        self.stigmem.reset()\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        self = BaseLayer.to(self, *args, **kwargs)\n",
    "        \n",
    "        self.stigmem = self.stigmem.to(*args, **kwargs)\n",
    "        self.normalization_layer_mark = self.normalization_layer_mark.to(*args, **kwargs)\n",
    "        self.normalization_layer_tick = self.normalization_layer_tick.to(*args, **kwargs)\n",
    "\n",
    "        self.init_recurrent = self.init_recurrent.to(*args, **kwargs)\n",
    "        self.recurrent = self.recurrent.to(*args, **kwargs)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.Recurrent = SRNN_Encoder(input_size, output_size, hidden_layers = 2 , hidden_dim = hidden_size).to(device)\n",
    "\n",
    "    def forward(self, input_enc):\n",
    "        \n",
    "        #input_enc = input_enc.detach()\n",
    "        output, mark, tick = self.Recurrent(input_enc)\n",
    "        return output, mark, tick\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.Recurrent.reset()\n",
    "        \n",
    "        \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.Recurrent = SRNN_Decoder(input_size, hidden_size, hidden_size = input_size, hidden_layers= 2 , hidden_dim = hidden_size).to(device)\n",
    "        self.out = nn.Linear(hidden_size, 10)\n",
    "        self.activacion_1 = torch.nn.PReLU() #exp_decay()\n",
    "        self.out2 = torch.nn.Linear(10, output_size)\n",
    "        \n",
    "    def forward(self, deposit, removal):\n",
    "        deposit = deposit.clone().detach().requires_grad_(False)\n",
    "        removal = removal.clone().detach().requires_grad_(False)\n",
    "        #print('mark:', deposit)\n",
    "        #print('tick:', removal)\n",
    "\n",
    "        #deposit = F.relu(deposit)\n",
    "        #removal = F.relu(removal)\n",
    "        output, mark, tick = self.Recurrent(deposit, removal)\n",
    "        output = self.out(output)\n",
    "        output = self.activacion_1(output)\n",
    "        output = self.out2(output)\n",
    "        \n",
    "        return output, mark, tick\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.Recurrent.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
